{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd35c6d-6b4e-4662-8e51-a28395c9e432",
   "metadata": {},
   "source": [
    "### Helper functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4afbfa5-065d-4441-b2d7-4cccb7e9a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 26 12:25:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:15:00.0  On |                  Off |\n",
      "| 34%   59C    P2    74W / 260W |   3900MiB / 48578MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:2D:00.0 Off |                  Off |\n",
      "| 41%   65C    P2    90W / 260W |   1484MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3056902      C   ...rain/anaconda3/bin/python     1339MiB |\n",
      "|    0   N/A  N/A   3087819      C   ...rain/anaconda3/bin/python     1237MiB |\n",
      "|    0   N/A  N/A   3093886      C   ...rain/anaconda3/bin/python     1321MiB |\n",
      "|    1   N/A  N/A   2700867      C   ...rain/anaconda3/bin/python     1481MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b92c75-a728-4945-a579-7b583128f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "\n",
    "# for training on my GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# lambda used later for fitness function\n",
    "LBD = 0.01\n",
    "\n",
    "# This module reshapes an input (tensor -> matrix).\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.size = size # a list\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == np.prod(self.size)\n",
    "        return x.view(x.shape[0], *self.size)\n",
    "# This module flattens an input (matrix -> tensor) by blending dimensions \n",
    "# beyond the batch size.\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "# The training procedure\n",
    "def training(max_patience, num_epochs, model, optimizer, training_loader, val_loader, verbose=False):\n",
    "    nll_val = []\n",
    "    error_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main training loop\n",
    "    for e in range(num_epochs):\n",
    "        model.train() # set the model to the training mode\n",
    "    # load batches\n",
    "        for indx_batch, (batch, targets) in enumerate(training_loader):\n",
    "            # for training on gpu, pass it there\n",
    "            batch, targets = batch.to(device), targets.to(device)\n",
    "            # calculate the forward pass (loss function for given images and labels)\n",
    "            loss = model.forward(batch, targets)\n",
    "            # remember we need to zero gradients! Just in case!\n",
    "            optimizer.zero_grad()\n",
    "            # calculate backward pass\n",
    "            loss.backward(retain_graph=True)\n",
    "            # run the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation: Evaluate the model on the validation data\n",
    "        nll_val, error_val = evaluation(val_loader, model_best=model, epoch=e, verbose=verbose)\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    return nll_val, error_val\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None, verbose = False):\n",
    "  # If available, load the best performing model\n",
    "    if model_best is None:\n",
    "        model_best = torch.load(name + '.model')\n",
    "  \n",
    "    model_best.eval()# set the model to the evaluation mode\n",
    "    loss_test = 0.\n",
    "    loss_error = 0.\n",
    "    N = 0.\n",
    "    # start evaluation\n",
    "    for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "        # evalutation on gpu\n",
    "        test_batch = test_batch.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "        # loss (nll)\n",
    "        loss_test_batch = model_best.forward(test_batch, test_targets, reduction='sum')\n",
    "        loss_test = loss_test + loss_test_batch.item()\n",
    "        # classification error\n",
    "        y_pred = model_best.classify(test_batch)\n",
    "        # for evalution on gpu\n",
    "        y_pred = y_pred.to(device)\n",
    "        e = 1.*(y_pred == test_targets)\n",
    "        loss_error = loss_error + (1. - e).sum().item()\n",
    "        # the number of examples\n",
    "        N = N + test_batch.shape[0]\n",
    "    # divide by the number of examples\n",
    "    loss_test = loss_test / N\n",
    "    loss_error = loss_error / N\n",
    "    # Print the performance\n",
    "    if epoch is None and verbose:\n",
    "        print(f'-> FINAL PERFORMANCE: nll={loss_test}, ce={loss_error}')\n",
    "    else:\n",
    "        if epoch % 1 == 0 and verbose:\n",
    "            print(f'Epoch: {epoch}, val nll={loss_test}, val ce={loss_error}')\n",
    "    return loss_test, loss_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a4bd8-ffd0-4042-9a78-d07a05db97f6",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d53dbcb-c173-49f8-b30e-0c37f29ff045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data created\n",
      "val data created\n",
      "test data created\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torchvision.transforms as Transformer\n",
    "\n",
    "class Housenumbers(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):       \n",
    "        # copied from original class, does not have influence on my code.\n",
    "        self.transforms = transforms \n",
    "        \n",
    "        # pull training data from matlab files using self-made function\n",
    "        train_pixels, train_targets = self.load_svhn(os.getcwd())\n",
    "        test_pixels, test_targets = self.load_svhn(os.getcwd(),split='test')\n",
    "        \n",
    "        # randomly splitting off validation data used to test performance during training\n",
    "        # placing randomstate = 42 to ensure that the same set of validation is used \n",
    "        # in the case that the class is instantiated several times\n",
    "        train_pixels, val_pixels, train_targets, val_targets = train_test_split(\n",
    "                                                            train_pixels,\n",
    "                                                            train_targets,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)\n",
    "        # store training data\n",
    "        if mode == 'train':\n",
    "            self.data = train_pixels\n",
    "            self.targets = train_targets\n",
    "        # store validation data\n",
    "        elif mode == 'val':\n",
    "            self.data = val_pixels\n",
    "            self.targets = val_targets\n",
    "        # store testing data\n",
    "        elif mode == 'test':\n",
    "            self.data = test_pixels\n",
    "            self.targets = test_targets\n",
    "        # catch case that unknown mode has been entered\n",
    "        else:\n",
    "            raise Exception('Unkown mode. Cannot create data.')\n",
    "        print(f'{mode} data created')\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_x = self.data[idx]\n",
    "        sample_y = self.targets[idx]\n",
    "        if self.transforms:\n",
    "            sample_x = self.transforms(sample_x)\n",
    "        return (sample_x, sample_y)\n",
    "    \n",
    "    # function that takes in a dataset in the form of\n",
    "    # [N, pic_height, pic_width, nr_dim] (numpy format)\n",
    "    # and returns \n",
    "    # [N, nr_dim, pic_height, pic_width] (torch format)\n",
    "    # additionally, all values are transformed to floats\n",
    "    def transform_to_torch_tensor(self, dataset):\n",
    "            # use transformer module as basis for tensor transformation\n",
    "            transform = Transformer.ToTensor()\n",
    "            l = [transform(i).float() for i in dataset]\n",
    "            return torch.stack(l)\n",
    "    \n",
    "    # function that reads the training data from the downloaded files\n",
    "    def load_svhn(self, image_dir, split='train'):\n",
    "        # depending on the split, open the two different files\n",
    "        image_file = 'train_32x32.mat' if split=='train' else 'test_32x32.mat'\n",
    "        image_dir = os.path.join(image_dir, image_file)\n",
    "        # open file in matlab format\n",
    "        matlab_labels = scipy.io.loadmat(image_dir)\n",
    "        # transform to numpy\n",
    "        labels = matlab_labels['y'].reshape(-1)\n",
    "        # set labels==10 to labels 0\n",
    "        labels[np.where(labels==10)] = 0\n",
    "        # transpose to numpy format\n",
    "        images_np = np.transpose(matlab_labels['X'], [3, 0, 1, 2]) / 255\n",
    "        # transpose to torch format -> tensor\n",
    "        torch_pixels = self.transform_to_torch_tensor(images_np)\n",
    "        \n",
    "        return torch_pixels, labels\n",
    "# create data instances for the different modes used during training and\n",
    "# testing of the CNN\n",
    "train_data = Housenumbers(mode='train')\n",
    "val_data = Housenumbers(mode='val')\n",
    "test_data = Housenumbers(mode='test')\n",
    "# load the created datasets into the DataLoader that provides the data\n",
    "# during both the training as well as the testing process\n",
    "training_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc976e6-b433-4f7a-8402-48228ca3d1c1",
   "metadata": {},
   "source": [
    "### CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1925534-61ae-41ba-938f-d9a9f9ad8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WEIGHTS = 3280342 #biggest amount of trainable parameters obtained \n",
    "# by calculating the model created by the maximum feature vector ([2,1,0,1,9,0])\n",
    "\n",
    "# -> training hyperparams\n",
    "lr = 1e-3 # learning rate\n",
    "wd = 1e-5 # weight decay\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for \n",
    "                  # longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47aa67e7-698f-4268-b4e7-363cbf46dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, classnet):\n",
    "        # init super (parent) class\n",
    "        super().__init__()\n",
    "        # import set of layers that was designed\n",
    "        self.classnet = classnet\n",
    "        # import loss function\n",
    "        self.nll = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    # This function classifies an image x to a class.\n",
    "    def classify(self, x):\n",
    "        # based on inputs x, a class prediction y is produced \n",
    "        # my multiplying the differnt layers based on the weights\n",
    "        y_pred = self.classnet(x)\n",
    "        # get a label in format long from the prediction\n",
    "        y_preds = torch.tensor([torch.argmax(pred) for pred in y_pred])\n",
    "        return y_preds # returns label\n",
    "\n",
    "    # This class outputs a value of the loss function.\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "        # get a label prediction of the set of layers that was created\n",
    "        y_pred = self.classnet(x)\n",
    "        # calculate the distance between calculated probabilities y_pred\n",
    "        # and onehot vector y that encodes the actual label of the picture\n",
    "        loss = self.nll(y_pred,y)\n",
    "        if reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34b882-f6b5-4a5e-b1a3-2ebe3835b211",
   "metadata": {},
   "source": [
    "### Feature Vector Class\n",
    "\n",
    "This class defines an integer representation in form of a vector. Each position in the vector encodes a different design option for the CNN. This part was created in colaboration with Antoni Stroinski, another AI student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673864e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVector():\n",
    "    def __init__(self, features:list):\n",
    "        # validate if the feature list is within the legal bounds for the design options\n",
    "        self.validation(features)\n",
    "        # maps the design options using a dictionary representation\n",
    "        mapping = self.mapping()\n",
    "        self.input_shape = 32\n",
    "        self.nr_classes = 10\n",
    "        # all design choices are drawn from the mapping dictionary using the feature vector\n",
    "        self.nr_filters = mapping['nr_filter'][features[0]] \n",
    "        self.conv_options = mapping['conv_options'][features[1]] \n",
    "        self.activation_function1 = mapping['activation_function1'][features[2]] \n",
    "        self.pooling_layer = mapping['pooling_options']['layer'][features[3]]\n",
    "        self.pooling_kernel_size = mapping['pooling_options']['kernel_size'][features[3]]\n",
    "        self.nr_nodes = mapping['nr_nodes'][features[4]] \n",
    "        self.activation_function2 = mapping['activation_function2'][features[5]]\n",
    "        self.list_vec = features\n",
    "\n",
    "    # method returning a dictionary data structure representing all available design options for the CNN\n",
    "    def mapping(self):\n",
    "        return {\n",
    "            'nr_filter': \n",
    "            {\n",
    "                0:8, \n",
    "                1:16,\n",
    "                2:32\n",
    "            },\n",
    "            'conv_options':\n",
    "            {\n",
    "                0:{'kernel':(3,3), 'padding':1},\n",
    "                1:{'kernel':(5,5), 'padding':2}\n",
    "            },\n",
    "            'activation_function1':\n",
    "            {\n",
    "                0:nn.ReLU(inplace=True),\n",
    "                1:nn.Sigmoid(),\n",
    "                2:nn.Tanh(),\n",
    "                3:nn.Softplus(),\n",
    "                4:nn.ELU(inplace=True)\n",
    "            },\n",
    "            'pooling_options':\n",
    "            {\n",
    "                'layer':\n",
    "                    {\n",
    "                        0:nn.MaxPool2d(kernel_size=(1,1)),\n",
    "                        1:nn.AvgPool2d(kernel_size=(1,1)),\n",
    "                        2:nn.MaxPool2d(kernel_size=(2,2)),\n",
    "                        3:nn.AvgPool2d(kernel_size=(2,2))\n",
    "                    },\n",
    "                'kernel_size':\n",
    "                    {\n",
    "                         0:1,\n",
    "                         1:1,\n",
    "                         2:2,\n",
    "                         3:2   \n",
    "                    }\n",
    "            },\n",
    "            'nr_nodes':\n",
    "            {\n",
    "                0: 10,\n",
    "                1: 20,\n",
    "                2: 30,\n",
    "                3: 40,\n",
    "                4: 50,\n",
    "                5: 60,\n",
    "                6: 70,\n",
    "                7: 80,\n",
    "                8: 90,\n",
    "                9: 100\n",
    "            },\n",
    "            'activation_function2':\n",
    "            {\n",
    "                0:nn.ReLU(inplace=True),\n",
    "                1:nn.Sigmoid(),\n",
    "                2:nn.Tanh(),\n",
    "                3:nn.Softplus(),\n",
    "                4:nn.ELU(inplace=True)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # creates a Sequential class object by iterating over the list of layers\n",
    "    def create_sequential(self):\n",
    "        return nn.Sequential(*[\n",
    "                                nn.Conv2d(3,self.nr_filters,kernel_size=self.conv_options['kernel'],\n",
    "                                          padding=self.conv_options['padding']),\n",
    "                                self.activation_function1,\n",
    "                                self.pooling_layer,\n",
    "                                Flatten(),\n",
    "                                # the number of inputs for the linear layer is dynamic and must be calculated in an extra method\n",
    "                                nn.Linear(self.calculate_flatten_size(),self.nr_nodes),\n",
    "                                self.activation_function2,\n",
    "                                nn.Linear(self.nr_nodes,self.nr_classes),\n",
    "                                nn.LogSoftmax(dim=1)\n",
    "                            ])\n",
    "\n",
    "    # method for calculating number of output features returned from pooling layer\n",
    "    def calculate_flatten_size(self):\n",
    "        # formulas taken from pytorch documentation\n",
    "        conv_out = self.input_shape + 2 *self.conv_options['padding'] -1 * (self.conv_options['kernel'][0] - 1)  -1 + 1\n",
    "        pooling_out = int(((conv_out + 2 * 0 - 1 * (self.pooling_kernel_size -1) -1)/self.pooling_kernel_size +1)**2*self.nr_filters)\n",
    "        return pooling_out\n",
    "\n",
    "    # method to validate if vector within bounds at all indices\n",
    "    def validation(self, features):\n",
    "        assert (0 <= features[0] <= 2), features[0]\n",
    "        assert (0 <= features[1] <= 1), features[1]\n",
    "        assert (0 <= features[2] <= 4), features[2]\n",
    "        assert (0 <= features[3] <= 3), features[3]\n",
    "        assert (0 <= features[4] <= 9), features[4]\n",
    "        assert (0 <= features[5] <= 4), features[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cfd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method returning total number of trainable features of a model\n",
    "def total_weights(m):\n",
    "    return sum(i.numel() for i in m.parameters() if i.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d33af288-9c89-45f0-af7e-a3db59012648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822742"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights(CNN(FeatureVector([2,1,4,2,9,4]).create_sequential()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de83db8a-b4cd-426e-9f2a-fcfa835a6713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822742"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights(CNN(FeatureVector([2,1,2,2,9,4]).create_sequential()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6201536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(feature_list, num_epochs = 10, verbose = False):\n",
    "    # creating a feature vector from a list of features\n",
    "    vec = FeatureVector(feature_list)\n",
    "    # creating a nn.Sequential() object based on feature specifications\n",
    "    classnet = vec.create_sequential()\n",
    "    # define model based on Sequential() object storing layers\n",
    "    model = CNN(classnet)\n",
    "    # sending model data to GPU for faster computation\n",
    "    model.to(device)\n",
    "    # initializing the optimizer\n",
    "    optimizer = torch.optim.Adamax([p for p in model.parameters() \n",
    "                                if p.requires_grad == True], lr=lr, weight_decay=wd) \n",
    "    # training the model for a number of epochs\n",
    "    # only the error_val matters in this problem because that is part of the objective function\n",
    "    _, error_val = training(max_patience=max_patience,\n",
    "                            num_epochs=num_epochs,\n",
    "                            model=model,\n",
    "                            optimizer=optimizer,\n",
    "                            training_loader=training_loader,\n",
    "                            val_loader=val_loader,\n",
    "                            verbose=verbose)\n",
    "    return error_val, total_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d652500-9821-42b3-a044-29c3023e1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA(object):\n",
    "    def __init__(self,test_performance, pop_size, bounds_min=None, \n",
    "                 bounds_max=None, mutation_prob=0.5, num_epochs=1,\n",
    "                 verbose=False):\n",
    "        self.verbose = verbose\n",
    "        # inheriting function for testing CNN performance\n",
    "        self.test_performance = test_performance\n",
    "        # hyperparameters for the algorithm\n",
    "        # pop size must be a round number due to parent recombination method\n",
    "        assert (pop_size%2==0)\n",
    "        self.pop_size = pop_size\n",
    "        self.bounds_min = bounds_min\n",
    "        self.bounds_max = bounds_max\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.num_epochs = num_epochs\n",
    "  \n",
    "    # draws random population, computes opposite population,\n",
    "    # returns union of both\n",
    "    # this function returns more individuals than pop_size but in the first\n",
    "    # parent_selection step, only n=pop_size individuals are drawn, so this \n",
    "    # does not matter for later\n",
    "    def initialize_population(self,optimized=False):\n",
    "        x = np.random.uniform(low=self.bounds_min, high=self.bounds_max, \n",
    "                              size=(self.pop_size, len(self.bounds_min)))\n",
    "        x = np.round(x)\n",
    "        if optimized:\n",
    "            opposite_vectors = []\n",
    "            for vector in x:\n",
    "                opposite_vector = self.bounds_max - vector\n",
    "                opposite_vectors.append(opposite_vector)\n",
    "            x_proposed = np.concatenate([x,opposite_vectors])\n",
    "            f_proposed = self.evaluate(x)\n",
    "            f_indices = f_proposed.argsort()\n",
    "            x_sorted = x_proposed[f_indices]\n",
    "            f_sorted = f_proposed[f_indices]\n",
    "            x = x_sorted[:self.pop_size]\n",
    "            f = f_sorted[:self.pop_size]\n",
    "            return x, f\n",
    "        elif not optimized:\n",
    "            f = self.evaluate(x)\n",
    "            return x, f\n",
    "        else:\n",
    "            raise Exception('optimized flag must be True or False.')\n",
    "\n",
    "    # selecting the n=pop_size best parents for offspring generation\n",
    "    def parent_selection(self, x_old, f_old):\n",
    "        f_inds = f_old.argsort()\n",
    "        x_old = x_old[f_inds]\n",
    "        f_old = f_old[f_inds]\n",
    "        x_parents = x_old[:self.pop_size]\n",
    "        f_parents = f_old[:self.pop_size]\n",
    "        return x_parents, f_parents\n",
    "    \n",
    "    # recombining parental genomes for offspring generation\n",
    "    # genes from two iteratively selected similarly performing parents are \n",
    "    # combined and crossover is performed at random locations\n",
    "    # forming two genotypes as a result\n",
    "    def recombination(self, x_parents, f_parents):\n",
    "        if self.verbose:\n",
    "            print('recombination: parents',x_parents)\n",
    "        children = []\n",
    "        for i in range(0,self.pop_size,2):\n",
    "            cross_over = np.random.randint(0,len(x_parents[i]))\n",
    "            child1 = x_parents[i][:cross_over].tolist()\n",
    "            child1.extend(x_parents[i+1][cross_over:].tolist())\n",
    "            child2 = x_parents[i+1][:cross_over].tolist()\n",
    "            child2.extend(x_parents[i][cross_over:].tolist())\n",
    "            children.append(child1)\n",
    "            children.append(child2)\n",
    "        x_children = np.array(children)\n",
    "        x_children = np.rint(x_children)\n",
    "        if self.verbose:\n",
    "            print('to children ', x_children,'\\n')\n",
    "        return x_children\n",
    "    \n",
    "    # function mutating children with p=mutation_prob\n",
    "    # if child is selected for mutation, a random gene is randomly \n",
    "    # altered to a new value within the bounds\n",
    "    def mutation(self, x_children):\n",
    "        if self.verbose:\n",
    "            print('mutation',x_children)\n",
    "        new_children = []\n",
    "        for child in x_children:\n",
    "            if np.random.uniform() > self.mutation_prob:\n",
    "                mutate = random.choice([0,1,2,3,4,5])\n",
    "                new_value = random.choice(range(bounds_min[mutate],bounds_max[mutate]+1))\n",
    "                child[mutate] = new_value\n",
    "            new_children.append(child)\n",
    "        new_children = np.asarray(new_children)\n",
    "        new_children = np.rint(new_children)\n",
    "        if self.verbose:\n",
    "            print('new chilren', new_children, '\\n')\n",
    "        return new_children\n",
    "\n",
    "    # n=pop_size survivors are selected based on performance\n",
    "    def survivor_selection(self, x_old, x_children, f_old, f_children):\n",
    "        x = np.concatenate([x_old, x_children])\n",
    "        f = np.concatenate([f_old, f_children])\n",
    "        f_inds = f.argsort()\n",
    "        x = x[f_inds]\n",
    "        f = f[f_inds]\n",
    "        x = x[:self.pop_size]\n",
    "        f = f[:self.pop_size]\n",
    "        return x, f\n",
    "\n",
    "    # performance of genotype (representing CNN structures) are evaluated based on training performance\n",
    "    # after n=num_epochs number of epochs\n",
    "    def evaluate(self, x):\n",
    "        performances = []\n",
    "        for i,feature_vec in enumerate(x):\n",
    "            ce, total_weights = self.test_performance(feature_vec,num_epochs = self.num_epochs)\n",
    "            objective = ce + LBD * total_weights / MAX_WEIGHTS\n",
    "            performances.append(objective)\n",
    "        return np.asarray(performances)\n",
    "  \n",
    "    # step function defining how a step in the EA is calculated\n",
    "    def step(self, x_old, f_old):\n",
    "        # selecting parents for offspring generation\n",
    "        x_parents, f_parents = self.parent_selection(x_old, f_old)\n",
    "        # recombining parental genotypes for offspring generation\n",
    "        x_children = self.recombination(x_parents, f_parents)\n",
    "        # mutating genotypes of children\n",
    "        x_children = self.mutation(x_children)\n",
    "        # calculating performance of children\n",
    "        f_children = self.evaluate(x_children)\n",
    "        # selecting survivors based on elitism\n",
    "        x, f = self.survivor_selection(x_old, x_children, f_old, f_children)\n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6133324-2a64-4cd1-b94c-bdd892f6f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if file already exists, overwrite content otherwise create new file\n",
    "def log_file_creation(path:str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        open(path, 'w').close()\n",
    "    else:\n",
    "        open(path, 'a').close()\n",
    "    # returns file path for convenience\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b74a9dd-2ed4-4d23-a40a-c85a7e6402fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[1. 1. 4. 1. 6. 3.] 0.1947174447174447\n",
      "[1. 1. 2. 3. 8. 3.] 0.19512694512694512\n",
      "[2. 1. 4. 1. 7. 0.] 0.20816270816270815\n",
      "[1. 0. 0. 0. 9. 1.] 0.21212121212121213\n",
      "[1. 0. 0. 0. 9. 0.] 0.21867321867321868\n",
      "[1. 0. 2. 3. 9. 0.] 0.23887523887523887\n",
      "[1. 1. 4. 1. 6. 1.] 0.3026208026208026\n",
      "[1. 0. 2. 3. 9. 1.] 0.32903357903357905\n",
      "Generation: 0, best fitness: 0.18\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[0. 1. 4. 1. 7. 4.] 0.1855036855036855\n",
      "[1. 1. 2. 3. 8. 3.] 0.18857493857493857\n",
      "[1. 1. 4. 1. 6. 3.] 0.1947174447174447\n",
      "[1. 1. 2. 3. 8. 3.] 0.19512694512694512\n",
      "[1. 1. 4. 1. 6. 3.] 0.19812994812994814\n",
      "[2. 1. 0. 0. 9. 1.] 0.20017745017745017\n",
      "[2. 1. 4. 1. 7. 0.] 0.20816270816270815\n",
      "Generation: 1, best fitness: 0.17\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[0. 1. 4. 1. 7. 4.] 0.1855036855036855\n",
      "[1. 1. 2. 3. 8. 3.] 0.18796068796068796\n",
      "[1. 1. 2. 3. 8. 3.] 0.18857493857493857\n",
      "[0. 1. 2. 2. 8. 3.] 0.19355719355719356\n",
      "[1. 1. 4. 1. 6. 3.] 0.1947174447174447\n",
      "[0. 1. 4. 1. 8. 3.] 0.1947174447174447\n",
      "Generation: 2, best fitness: 0.17\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[1. 1. 2. 2. 8. 3.] 0.18372918372918373\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[0. 1. 4. 1. 7. 4.] 0.1855036855036855\n",
      "[1. 1. 4. 2. 8. 0.] 0.1865956865956866\n",
      "[1. 1. 2. 3. 8. 3.] 0.18796068796068796\n",
      "[1. 1. 2. 1. 7. 4.] 0.18796068796068796\n",
      "[1. 1. 2. 3. 8. 3.] 0.18857493857493857\n",
      "Generation: 3, best fitness: 0.17\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[1. 1. 2. 2. 8. 3.] 0.18372918372918373\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[1. 1. 4. 2. 8. 0.] 0.18523068523068523\n",
      "[0. 1. 4. 1. 7. 4.] 0.1855036855036855\n",
      "[1. 1. 4. 2. 8. 0.] 0.1865956865956866\n",
      "[1. 1. 2. 3. 8. 4.] 0.18680043680043681\n",
      "Generation: 4, best fitness: 0.17\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 2. 2. 8. 3.] 0.17458367458367458\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[1. 1. 2. 2. 8. 3.] 0.18372918372918373\n",
      "[2. 1. 4. 1. 8. 3.] 0.1844799344799345\n",
      "[1. 1. 4. 2. 8. 0.] 0.18523068523068523\n",
      "[0. 1. 4. 1. 7. 4.] 0.1855036855036855\n",
      "Generation: 5, best fitness: 0.17\n",
      "[1. 1. 2. 2. 8. 3.] 0.16523341523341523\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 2. 2. 8. 3.] 0.17431067431067432\n",
      "[1. 1. 2. 2. 8. 3.] 0.17458367458367458\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[0. 1. 4. 2. 8. 2.] 0.1829101829101829\n",
      "[1. 1. 2. 3. 7. 4.] 0.1829101829101829\n",
      "[1. 1. 4. 1. 8. 3.] 0.1835926835926836\n",
      "Generation: 6, best fitness: 0.17\n",
      "[1. 1. 2. 2. 8. 3.] 0.16523341523341523\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 2. 2. 8. 3.] 0.1734916734916735\n",
      "[1. 1. 2. 2. 8. 3.] 0.17431067431067432\n",
      "[1. 1. 2. 2. 8. 3.] 0.17458367458367458\n",
      "[1. 1. 4. 1. 8. 3.] 0.1773136773136773\n",
      "[1. 1. 2. 2. 8. 3.] 0.17854217854217855\n",
      "[2. 1. 0. 2. 8. 3.] 0.1786104286104286\n",
      "Generation: 7, best fitness: 0.17\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[1. 1. 2. 2. 8. 3.] 0.16523341523341523\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.1728774228774229\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "[1. 1. 2. 2. 8. 3.] 0.1734916734916735\n",
      "[1. 1. 2. 2. 8. 3.] 0.17431067431067432\n",
      "[1. 1. 2. 2. 8. 3.] 0.17437892437892438\n",
      "Generation: 8, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.1625034125034125\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[1. 1. 2. 2. 8. 3.] 0.16523341523341523\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "[1. 1. 2. 2. 8. 3.] 0.1728774228774229\n",
      "[1. 1. 2. 2. 8. 3.] 0.17294567294567295\n",
      "Generation: 9, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.1625034125034125\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[2. 1. 2. 2. 8. 3.] 0.16373191373191373\n",
      "[2. 1. 2. 2. 8. 3.] 0.16407316407316408\n",
      "[1. 1. 2. 2. 8. 3.] 0.16523341523341523\n",
      "[2. 1. 2. 2. 8. 3.] 0.16564291564291564\n",
      "[1. 1. 2. 2. 8. 3.] 0.16844116844116844\n",
      "Generation: 10, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.16175266175266176\n",
      "[2. 1. 2. 2. 8. 3.] 0.16223041223041224\n",
      "[2. 1. 2. 2. 8. 3.] 0.1625034125034125\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "[2. 1. 2. 2. 8. 3.] 0.1627081627081627\n",
      "[2. 1. 2. 2. 8. 3.] 0.16373191373191373\n",
      "Generation: 11, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.16134316134316135\n",
      "[2. 1. 2. 2. 8. 3.] 0.16175266175266176\n",
      "[2. 1. 2. 2. 8. 3.] 0.16223041223041224\n",
      "[2. 1. 2. 2. 8. 3.] 0.1625034125034125\n",
      "[2. 1. 2. 2. 8. 3.] 0.16257166257166258\n",
      "Generation: 12, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 6. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.16072891072891074\n",
      "[2. 1. 2. 2. 8. 3.] 0.16134316134316135\n",
      "[2. 1. 2. 2. 8. 3.] 0.16175266175266176\n",
      "[2. 1. 2. 2. 8. 3.] 0.16223041223041224\n",
      "Generation: 13, best fitness: 0.16\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 6. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.16072891072891074\n",
      "[2. 1. 2. 2. 8. 3.] 0.16134316134316135\n",
      "Generation: 14, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15786240786240785\n",
      "[2. 1. 2. 2. 6. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "[2. 1. 2. 2. 8. 3.] 0.15943215943215944\n",
      "Generation: 15, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15786240786240785\n",
      "[2. 1. 2. 2. 8. 4.] 0.15813540813540813\n",
      "[2. 1. 2. 2. 6. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.15868140868140868\n",
      "[2. 1. 2. 2. 8. 3.] 0.1588861588861589\n",
      "Generation: 16, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "[2. 1. 2. 2. 8. 3.] 0.15717990717990718\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 3.] 0.15765765765765766\n",
      "[2. 1. 2. 2. 8. 3.] 0.15786240786240785\n",
      "Generation: 17, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "[2. 1. 2. 2. 8. 3.] 0.15717990717990718\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 4.] 0.1575894075894076\n",
      "Generation: 18, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "[2. 1. 2. 2. 8. 3.] 0.15717990717990718\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "[2. 1. 2. 2. 8. 3.] 0.15745290745290746\n",
      "[2. 1. 2. 2. 8. 4.] 0.1575894075894076\n",
      "Generation: 19, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "[2. 1. 2. 2. 8. 3.] 0.15717990717990718\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "Generation: 20, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "[2. 1. 2. 2. 8. 3.] 0.15717990717990718\n",
      "[2. 1. 2. 2. 8. 4.] 0.15738465738465737\n",
      "Generation: 21, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.1567021567021567\n",
      "Generation: 22, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 23, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 24, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 25, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 26, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 27, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 28, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 29, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "[2. 1. 2. 2. 8. 3.] 0.15656565656565657\n",
      "Generation: 30, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "Generation: 31, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "[2. 1. 2. 2. 8. 3.] 0.15636090636090635\n",
      "Generation: 32, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "[2. 1. 2. 2. 8. 3.] 0.15622440622440623\n",
      "Generation: 33, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15465465465465467\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "[2. 1. 2. 2. 8. 3.] 0.15615615615615616\n",
      "Generation: 34, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15465465465465467\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "Generation: 35, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15465465465465467\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "Generation: 36, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15465465465465467\n",
      "[2. 1. 2. 2. 8. 4.] 0.15492765492765492\n",
      "Generation: 37, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "[2. 1. 2. 2. 8. 4.] 0.15465465465465467\n",
      "Generation: 38, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "Generation: 39, best fitness: 0.15\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "[2. 1. 2. 2. 8. 3.] 0.15458640458640457\n",
      "Generation: 40, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "Generation: 41, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "Generation: 42, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "Generation: 43, best fitness: 0.15\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "[2. 1. 2. 2. 8. 4.] 0.15438165438165438\n",
      "Generation: 44, best fitness: 0.15\n",
      "[2. 1. 4. 2. 9. 4.] 0.1483073983073983\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "Generation: 45, best fitness: 0.15\n",
      "[2. 1. 4. 2. 9. 4.] 0.1483073983073983\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.15424515424515425\n",
      "Generation: 46, best fitness: 0.15\n",
      "[2. 1. 4. 2. 9. 4.] 0.1483073983073983\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 4.] 0.15267540267540267\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "Generation: 47, best fitness: 0.15\n",
      "[2. 1. 4. 2. 9. 4.] 0.1483073983073983\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 4.] 0.15267540267540267\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 4.] 0.1541769041769042\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "Generation: 48, best fitness: 0.15\n",
      "[2. 1. 4. 2. 9. 4.] 0.1483073983073983\n",
      "[2. 1. 2. 2. 8. 4.] 0.15001365001365002\n",
      "[2. 1. 2. 2. 8. 4.] 0.1517199017199017\n",
      "[2. 1. 2. 2. 9. 3.] 0.151992901992902\n",
      "[2. 1. 2. 2. 8. 4.] 0.15212940212940212\n",
      "[2. 1. 2. 2. 8. 4.] 0.15267540267540267\n",
      "[2. 1. 2. 2. 9. 4.] 0.15281190281190282\n",
      "[2. 1. 2. 2. 8. 3.] 0.1532896532896533\n",
      "[2. 1. 2. 2. 8. 3.] 0.15363090363090362\n",
      "[2. 1. 2. 2. 8. 3.] 0.1541769041769042\n",
      "Generation: 49, best fitness: 0.15\n",
      "FINISHED!\n"
     ]
    }
   ],
   "source": [
    "# calculating the processing time out of interest\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# initializing hyperparameters\n",
    "pop_size = 10\n",
    "bounds_min = [0,0,0,0,0,0]\n",
    "bounds_max = [2,1,4,3,9,4]\n",
    "num_generations = 50\n",
    "mutation_prob = 0.6\n",
    "num_epochs = 3\n",
    "optimized_population_initialization = True\n",
    "ea_verbose = False\n",
    "\n",
    "# file name for result writing during training on GPU because output is silent by necessity\n",
    "file_name = log_file_creation('results_final.txt')\n",
    "\n",
    "# initializing the EA algorithm\n",
    "ea = EA(test_performance, pop_size, bounds_min=bounds_min, bounds_max=bounds_max, num_epochs=num_epochs,verbose=ea_verbose)\n",
    "\n",
    "# initializing the population (union of random and opposite positions in random)\n",
    "# see elaboration of this method in the paper\n",
    "x, f = ea.initialize_population(optimized=optimized_population_initialization)\n",
    "\n",
    "# print(x)\n",
    "# print(f)\n",
    "\n",
    "# overwriting lists from last run of the EA\n",
    "f_best = None\n",
    "x_best = None\n",
    "\n",
    "# store best performing genotype and fitness in lists\n",
    "f_best = [f.min()]\n",
    "x_best = [x[np.where(f == f.min())]]\n",
    "\n",
    "# Run the EA for a number of generations\n",
    "for i in range(num_generations):\n",
    "    # do one step\n",
    "    x, f = ea.step(x, f)\n",
    "    for j in range(len(x)):\n",
    "        print(x[j],f[j])\n",
    "    # if a new minimum has been found\n",
    "    if f.min() < f_best[-1]:\n",
    "        # append that minimum and its genotype\n",
    "        f_best.append(f.min())\n",
    "        x_best.append(x[np.where(f == f.min())])\n",
    "    else: # reappend the old minimum and its genotype\n",
    "        f_best.append(f_best[-1])\n",
    "        x_best.append(x_best[-1])\n",
    "    # write results to file\n",
    "    with open(file_name,'a') as file:\n",
    "        best_vec = list(map(round,x_best[-1][0]))\n",
    "        file.write(f'Generation: {i}\\tbest fitness: {f.min():.6f}\\tbest vector: {best_vec}\\n')\n",
    "    print('Generation: {}, best fitness: {:.2f}'.format(i, f.min()))\n",
    "\n",
    "# store final results in variables for later analysis\n",
    "final_vec = FeatureVector(best_vec)\n",
    "\n",
    "# write EA-generated CNN architecture to file\n",
    "with open(file_name,'a') as file:\n",
    "    print('final CNN structure:\\n',final_vec.create_sequential(),file=file)\n",
    "    print(f'Total time of computing: {datetime.now() - start}.',file=file)\n",
    "print('FINISHED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e02323-439d-4173-ac9a-802d7043b93b",
   "metadata": {},
   "source": [
    "### Evaluation of the finally generated CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19d7c985-651e-42de-9a5e-fd7dad151f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=0.8032045957334635, val ce=0.23634998634998636\n",
      "Epoch: 5, val nll=0.4843323455786334, val ce=0.13247338247338247\n",
      "Epoch: 10, val nll=0.4987864180272504, val ce=0.1317908817908818\n",
      "Epoch: 15, val nll=0.524846283498762, val ce=0.13527163527163527\n",
      "Epoch: 20, val nll=0.5490436263579734, val ce=0.12974337974337974\n",
      "Epoch: 25, val nll=0.5683637124217016, val ce=0.128992628992629\n",
      "Epoch: 30, val nll=0.6083648572878013, val ce=0.12933387933387933\n",
      "-> FINAL PERFORMANCE: nll=0.8144579999522061, ce=0.15165949600491702\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def final_evaluation(test_loader, model, name=None, epoch=None):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    loss_test = 0.\n",
    "    loss_error = 0.\n",
    "    N = 0.\n",
    "    # start evaluation\n",
    "    for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "        # send data to GPU for faster calculations\n",
    "        test_batch, test_targets =test_batch.to(device), test_targets.to(device)\n",
    "        # calculate batch of loss values based on testing data\n",
    "        loss_test_batch = model.forward(test_batch, test_targets, reduction='sum')\n",
    "        loss_test = loss_test + loss_test_batch.item()\n",
    "        # predict class labels based on network features\n",
    "        y_pred = model.classify(test_batch)\n",
    "        # send data to GPU for faster calculation\n",
    "        y_pred = y_pred.to(device)\n",
    "        e = 1.*(y_pred == test_targets)\n",
    "        loss_error = loss_error + (1. - e).sum().item()\n",
    "        # the number of examples\n",
    "        N = N + test_batch.shape[0]\n",
    "    # divide by the number of examples\n",
    "    loss_test = loss_test / N\n",
    "    loss_error = loss_error / N\n",
    "    # Print the performance\n",
    "    if epoch is None:\n",
    "        print(f'-> FINAL PERFORMANCE: nll={loss_test}, ce={loss_error}')\n",
    "    else:\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch: {epoch}, val nll={loss_test}, val ce={loss_error}')\n",
    "    return loss_test, loss_error\n",
    "\n",
    "def final_training(max_patience, num_epochs, best_vec, training_loader, val_loader):\n",
    "    # based on the best found feature vector from the EA, define a fresh model for training\n",
    "    vec = FeatureVector(best_vec)\n",
    "    # create nn.Sequential() with embedded design choices\n",
    "    classnet = vec.create_sequential()\n",
    "    # create model from layer object\n",
    "    model = CNN(classnet)\n",
    "    # send model to GPU\n",
    "    model.to(device)\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adamax([p for p in model.parameters() \n",
    "                                    if p.requires_grad == True], lr=lr, weight_decay=wd) \n",
    "    # create list of negative log loss and classification error for plotting over time\n",
    "    nll_val = []\n",
    "    error_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "    # iterate over trainging epochs\n",
    "    for e in range(num_epochs):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        # load batches\n",
    "        for indx_batch, (batch, targets) in enumerate(training_loader):\n",
    "            # send data to GPU for faster calculation\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # calculate the forward pass (loss function for given images and labels)\n",
    "            loss = model.forward(batch, targets)\n",
    "            # remember we need to zero gradients! Just in case!\n",
    "            optimizer.zero_grad()\n",
    "            # calculate backward pass\n",
    "            loss.backward(retain_graph=True)\n",
    "            # run the optimizer\n",
    "            optimizer.step()\n",
    "        # return the final evaluation\n",
    "        loss_e, error_e = final_evaluation(val_loader, model=model, epoch=e)\n",
    "        # save for plotting\n",
    "        nll_val.append(loss_e)  \n",
    "        # save for plotting\n",
    "        error_val.append(error_e)\n",
    "        # save inital model\n",
    "        if e == 0:\n",
    "            torch.save(model, 'final.model')\n",
    "            best_nll = loss_e\n",
    "        # in later iterations\n",
    "        else:\n",
    "            # if there is a new best model found\n",
    "            if loss_e < best_nll:\n",
    "                torch.save(model, 'final.model')\n",
    "                best_nll = loss_e\n",
    "                patience = 0\n",
    "            else: # if there is no new best model found, wait longer\n",
    "                patience = patience + 1\n",
    "        # if no new model was found for a certain nr of generations, break the training loop\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    # Return nll and classification error arrays for plotting\n",
    "    nll_val = np.asarray(nll_val)\n",
    "    error_val = np.asarray(error_val)\n",
    "    return nll_val, error_val, model\n",
    "\n",
    "def plot_curve(name, signal, xlabel='epochs', ylabel='nll', \\\n",
    "               color='b-', test_eval=None):\n",
    "    # plot the curve\n",
    "    plt.plot(np.arange(len(signal)), signal, color, linewidth='3', label=ylabel +' val')\n",
    "    # if available, add the final (test) performance\n",
    "    if test_eval is not None:\n",
    "        plt.hlines(test_eval, xmin=0, xmax=len(signal), linestyles='dashed', label=ylabel\\\n",
    "               +' test')\n",
    "        plt.text(len(signal), test_eval, \"{:.3f}\".format(test_eval),)\n",
    "    # set x- and ylabels, add legend, save the figure\n",
    "    plt.xlabel(xlabel), plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def final_pipeline(final_vec,num_epochs):\n",
    "    nll_val, error_val, trained_model = final_training(best_vec=final_vec.list_vec,\n",
    "                                                max_patience=max_patience,\n",
    "                                                num_epochs=num_epochs,\n",
    "                                                training_loader=training_loader,\n",
    "                                                val_loader=val_loader)\n",
    "    test_loss, test_error = final_evaluation(model=trained_model, test_loader=test_loader)\n",
    "    plot_curve('final.model', nll_val, ylabel='nll', test_eval=test_loss)\n",
    "    plot_curve('final.model', error_val, ylabel='ce', color='r-', test_eval=test_error)\n",
    "final_pipeline(final_vec=FeatureVector([2,1,4,2,9,4]),num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
